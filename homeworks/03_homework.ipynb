{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc5aIKvyyOkG"
      },
      "source": [
        "За последнее домашнее задание можно набрать 50 баллов – *таким образом, можно набрать до 5 дополнительных баллов*, которые могут помочь добрать где-либо ранее потерянные баллы, если есть такая необходимость.\n",
        "\n",
        "**Дедлайн – 24 декабря, 23:59**. *Крайне приветствуется* сдача в ранний срок – работа будет проверена практически сразу (в рабочие часы) и студенту будет выставлены итоговые баллы**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B4ww11_Wdw5"
      },
      "source": [
        "**1.** (5 баллов) Вы разработали модель, которая страдает низким смещением и высокой дисперсией. Как Вы думаете, какой метод/алгоритм можно в таком случае применить и почему? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для начала надо понять, что является основными источниками высокой дисперсии в финальных предсказаниях модели. В первую очередь, это:\n",
        "1. шум в данных для обучения модели;\n",
        "2. использования случайных шагов в алгоритмах обучения (примерами таких моделей может послужить Random Forest или модели на основе стахостического градиентного спуска).\n",
        "\n",
        "\n",
        "Чтобы понизить дисперсию (variance) модели нам надо увеличить ее смешение (bias). Для этого мы можем, например, обучить модель несколько раз на разных маленьких семплах данных и взять среднию оценку, опираясь на центральную предельную теорему. Для этого мы можем, например, использовать ансамбль моделей (обучить несколько моделей на маленьких смеплах и брать среднее из их предсказаний) или ансамбль параметров (например, обучить на разных маленьких семплах несколько линейных регрессий и взять среднее их весов).  Мы увеличим таким образом смешение, предположив, что среднее значение будет более точным. Второй вариант - это значительно увеличить объем данных для обучения. Здесь мы опираемся на Закон больших чисел."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWtaoeHFWeAA"
      },
      "source": [
        "**2.** (5 баллов) У Вас есть датафрейм `df_baguettes`, содержащий список цен на французские багеты. Но оказалось, что в этом датафрейме отсутствуют многие значения в столбце цен. По крайней мере несколько багетов имеют цену в столбце.\n",
        "Напишите функцию `median_baguettes`, которая вычисляет медианную цену выбранных французских багетов вместо отсутствующих значений. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def median_baguettes(baguettes):\n",
        "    baguettes = baguettes.dropna().sort_values('price')\n",
        "    number_rows = baguettes.shape[0]\n",
        "    if number_rows % 2 == 0:\n",
        "        median = (baguettes.iloc[number_rows // 2] + baguettes.iloc[number_rows // 2 - 1]) / 2\n",
        "    else:\n",
        "        median = baguettes.iloc[number_rows // 2]\n",
        "\n",
        "    return baguettes.fillna(median)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Тесты*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>correct price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32.0</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   price  correct price\n",
              "6    5.0            5.0\n",
              "5   10.0           10.0\n",
              "0   20.0           20.0\n",
              "2   32.0           32.0\n",
              "1    NaN           15.0\n",
              "3    NaN           15.0\n",
              "4    NaN           15.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Even number of elements\n",
        "df_baguettes = pd.DataFrame([20, None, 32, None, None, 10, 5], columns=['price'])\n",
        "correct = df_baguettes.fillna(df_baguettes.median()).rename(columns={'price': 'correct price'})\n",
        "pd.concat([median_baguettes(df_baguettes), correct], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>correct price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32.0</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   price  correct price\n",
              "5    5.0            5.0\n",
              "0   20.0           20.0\n",
              "2   32.0           32.0\n",
              "1    NaN           20.0\n",
              "3    NaN           20.0\n",
              "4    NaN           20.0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Odd number of elements\n",
        "df_baguettes = pd.DataFrame([20, None, 32, None, None, 5], columns=['price'])\n",
        "correct = df_baguettes.fillna(df_baguettes.median()).rename(columns={'price': 'correct price'})\n",
        "pd.concat([median_baguettes(df_baguettes), correct], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>correct price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   price  correct price\n",
              "3    5.0            5.0\n",
              "0    NaN            5.0\n",
              "1    NaN            5.0\n",
              "2    NaN            5.0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# One element\n",
        "df_baguettes = pd.DataFrame([None,  None, None, 5], columns=['price'])\n",
        "correct = df_baguettes.fillna(df_baguettes.median()).rename(columns={'price': 'correct price'})\n",
        "pd.concat([median_baguettes(df_baguettes), correct], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4vriMdTZrpW"
      },
      "source": [
        "**3.** (5 баллов) Предположим, что у Вас есть набор данных как с непрерывными, так и с категориальными переменными. Какие методы кластеризации не помогут достичь высокого качества построенной модели при описанной ситуации с данными и почему? И какие методы кластеризации Вы бы использовали? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Пример модели кластеризации, которая может плохо работать с данными, в которых встречаются категориальные и непрерывные признаки - это KMeans. Это происходит, потому что KMeans плохо работает с признаками из разных шкал, и для нас в целом часто не очень очевидно, как посчитать расстояние между категориальными признаками. Этот эффект можно преодолеть, если предобработать категориальные признаки и граммотно подобрать метрику близости. Например, Евклидова метрика часто бывает не лучшим вариантом для данных с категориальными признаками, выбор метрики сильно зависит от данных, и с ней лучше всего поэксперементировать. Неплохим вариантом метрики может стать Gower Distance. Если вам не подошел KMeans с метрикой Gower Distance, например, из-за большой размерности данных, то стоит попробовать алгоритмы на основе Dimensionality Reduction, например, PCA или его обобщение FAMD (Factorial Analysis of Mixed Data). FAMD отличается предварительной преобработкой данных перед использвоанием PCA, мы сначала используем hot encoded для наших категориальных признаков и нормализуемые числовые признаки (вычитаем среднее и делим на стандартное отклонение). Затем мы так же нормализуем категориальные признаки после hot encoded, но немного по-другому, они делятся на квадрытный корень отношения колличества элементов с этим категориальным признаком ко всем элементам и центрируются (вычитаем среднее значение). Потом запускаем PCA на предобработанных данных. Есть еще ряд техник на основе Dimensionality Reduction, например, UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction). Также мы можем обратиться к модификациям K-Means: K-Modes и K-prototypes. В конечном итоге выбор алгоритма сильно зависит от ваших данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKfQdhKceSY-"
      },
      "source": [
        "**4.** Предположим, что перед Вами стоит задача: построить модель машинного обучения для прогнозирования заполняемости отеля в любую дату.\n",
        "\n",
        "\n",
        "*   (2.5 баллов) Какие данные Вы бы использовали для обучения вашей модели? \n",
        "*   (10 баллов со скелетом кода, 5 баллов с письменным описанием) Какую бы модель Вы разработали? \n",
        "*   (2.5 баллов) Как бы Вы оценивали качество модели? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Данные\n",
        "Для прогнозирования заполнености отеля логично использовать данные о времени туристического сезона в стране, где находится отель. Например, в Дубаях туристический сезон - это в основном зимнее время, когда там не так жарко, в Черногорию лучше приезжать в середине Августа. Для этого мы можем ввести категориальный признак для того, какой сейчас сезон, причем необязательно бинарный. Также важным признаком является цена проживания, которая чаще всего меняется для разных дат, и страна. Дополнительно можно добавить несколько бинарных признаков для учета доступности рахных услуг отеля. Например, если это горнолыжный отель, то нам нужно учитывать, есть ли в определенные даты возможность кататься посетителям на лыжах, или она не доступна из-за сезона, при этом одноверменно может быть доступна другая услуга, напрмер, горные прогулки.\n",
        "\n",
        "### Модель\n",
        "Так как у нас используются и категориальные, и численные признаки, то нам нужна модель, которая будет работать для смешанных данных. Неплохим вариантом может стать SVM с использованием Direct numerical encoding. Мы будем использовать именно Direct numerical encoding, так как в выбранных нами данных есть порядок, какое значение более приоритетное, чем другое.\n",
        "\n",
        "### Оценки точности\n",
        "Классическим вариантом для задач такого типа будет MAE (Mean Absolute Error) и RMSE (Root Mean Squared Error). Первое поможет нам оценить, насколько наши прогнозы далеки от реальных в среднем, а вторая метрика отличается одинаковой шкалой с нашим параметром (что часто удобно для понимания) и поможет нам бороться с большими ошибками. Как мне кажется, RMSE для нас даже более приоритетная метрика, так как небольшие ошибки сильно не навредят нашей бизнес цели. Но если наше предсказание будет давать переодически крупные отклонения, то из-за этого отель может сильно недооценить поток клиентов и не быть готовым или вложить излишние деньги и усилия, когда приедет сильно меньше гостей, чем предполагалось. Также важно учитывать, что если модель ошибается в меньшую сторону, то для нас это лучше, чем если она ошибается в большую, потому что для отеля нехватка мест значительно хуже, чем их избыток. Поэтому имеет смысл рассмотреть применение весов к подсчету MAE и RMSE для того, чтобы придать больший вес положительным оценкам и сместить нашу модель. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# This step Encode categorical values, important to have labels in order.\n",
        "# Better labels should be in the end of array this will help us to \n",
        "# give bigger value for better labels\n",
        "for feature, feature_labels in categorical_features:\n",
        "    le = LabelEncoder(feature_labels)\n",
        "    feature = le.fit(feature)\n",
        "\n",
        "# Fit our model with standart scaler first\n",
        "regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "prediction = regr.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, prediction)\n",
        "rmse = mean_squared_error(y_test, prediction, squared=False)\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Комментарии к коду\n",
        "Дополнительно стоит рассмотреть вариант использования подхода FAMD для смешанных данных, а также использовать GRidSearchCV для подбора гиперпараметров и тестирования различных ядер."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm4nAmkycnfc"
      },
      "source": [
        "**5.** (10 баллов) У Вас есть строка, напоминающая объявление списка словарей. Не используя Pandas, напишите функцию `read_split_from_str`, которая:\n",
        "\n",
        "*   Читает данные и кодирует их как список словарей (данные можно сгенерировать случайно при помощи numpy, например);\n",
        "*   Разделяет данные на два списка: train и test в соотношении 70:30 и, соответственно, возвращает список `[training_set,testing_set]`. \n",
        "\n",
        "Пример входных данных:\n",
        "\n",
        "`list_of_dict_string = \"[{'x': 0, 'y': 4}, {'x': 20, 'y': 104}, {'x': 128, 'y': 212}]\"`\n",
        "\n",
        "Пример выходных данных:\n",
        "\n",
        "```\n",
        "def read_split_from_str(list_of_dict_string) \n",
        "    [\n",
        "    [{'x': 0, 'y': 4}, {'x': 20, 'y': 104}],\n",
        "    [{'x': 128, 'y': 212}]\n",
        "    ]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[{'x': 0.0, 'y': 4.0}, {'x': 20.0, 'y': 104.0}], [{'x': 128.0, 'y': 212.0}]]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "test = \"[{'x': 0, 'y': 4}, {'x': 20, 'y': 104}, {'x': 128, 'y': 212}]\"\n",
        "\n",
        "def read_split_from_str(list_of_dict_string: str):\n",
        "    \"\"\"\n",
        "    Implementation in pure python without any third-party libraries.\n",
        "    It may be better to use sklearn split for testing and training and np.arrays instead of lists.\n",
        "    \"\"\"\n",
        "    match = re.findall(\"{.*}\", list_of_dict_string)\n",
        "    dicts = re.split(r\"\\s*}\\s*,\\s*{\\s*\", match[0][1:-1])\n",
        "    result = []\n",
        "    for dict_str in dicts:\n",
        "        dict_obj = {}\n",
        "        pairs = re.split(r\"\\s*,\\s*\", dict_str)\n",
        "        for pair in pairs:\n",
        "            try:\n",
        "                key, value = re.split(r\"\\s*:\\s*\", pair)\n",
        "            except ValueError:\n",
        "                raise Exception(\"Incorrect syntax\")\n",
        "\n",
        "            try:\n",
        "                value = float(value.strip())\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            try:\n",
        "                key = float(key.strip())\n",
        "            except ValueError:\n",
        "                key = key.replace(\"'\", '')\n",
        "            \n",
        "\n",
        "            dict_obj[key] = value \n",
        "        \n",
        "        result.append(dict_obj)\n",
        "\n",
        "    n_train = int(len(result)*0.7)\n",
        "    \n",
        "    return [result[:n_train], result[n_train:]]\n",
        "\n",
        "\n",
        "read_split_from_str(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2NbvSMrdvZY"
      },
      "source": [
        "**6.** (10 баллов) Разработайте модель классификации k-ближайших соседей не используя scikit-learn, соблюдая некоторые условия: \n",
        "\n",
        "\n",
        "*   В качестве метрики близости использовано евклидово расстояние;\n",
        "*   Модель обратаывает датафреймы произвольного количества строк/столбцов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_array, check_X_y, check_is_fitted, check_random_state\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "class ClassificationKNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, max_iter=100, random_state=None) -> None:\n",
        "        self.max_iter = max_iter\n",
        "        self.random_state = random_state\n",
        "        super().__init__()\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            \"max_iter\": self.max_iter,\n",
        "            \"random_state\": self.random_state\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        return super().set_params(**params)\n",
        "\n",
        "    def compute_distances(self, X):\n",
        "        distances = np.zeros((X.shape[0], self.num_clusters_))\n",
        "        for i in range(self.num_clusters_):\n",
        "            distances[:, i] = np.linalg.norm(X - self.centers_[i, :], ord=2, axis=1)\n",
        "        return distances\n",
        "\n",
        "    def compute_centers(self, X) -> bool:\n",
        "        new_centers = np.zeros((self.num_clusters_, X.shape[1]))\n",
        "        for i in range(self.num_clusters_):\n",
        "            new_centers[i, :] = np.mean(X[self.labels_ == i, :], axis=0)\n",
        "        \n",
        "        if np.all(new_centers == self.centers_):\n",
        "            return True\n",
        "        else:\n",
        "            self.centers_ = new_centers\n",
        "            return False\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.num_clusters_ = unique_labels(y).shape[0]\n",
        "\n",
        "        # init centers\n",
        "        self.random_state_ = check_random_state(self.random_state)\n",
        "        rand_indexes = self.random_state_.randint(X.shape[0], size=self.num_clusters_)\n",
        "        self.centers_ = X[rand_indexes]\n",
        "\n",
        "        # calculate centers\n",
        "        for i in range(self.max_iter):\n",
        "            distances = self.compute_distances(X)\n",
        "            self.labels_ = np.argmin(distances, axis=1)\n",
        "            if self.compute_centers(X):\n",
        "                break\n",
        "\n",
        "        # map labels from data with clusters centers\n",
        "        self.map_label_cluster_ = {}\n",
        "        for i in range(self.num_clusters_):\n",
        "            labels, count = np.unique(y[self.labels_ == i], return_counts=True)\n",
        "            self.map_label_cluster_[i] = labels[np.argmax(count)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X)\n",
        "        distances = self.compute_distances(X)\n",
        "        centers = np.argmin(distances, axis=1)\n",
        "\n",
        "        for i in range(centers.shape[0]):\n",
        "            centers[i] = self.map_label_cluster_[centers[i]]\n",
        "\n",
        "        return centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = ClassificationKNN(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "prediction = clf.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3c6f432044ad9b83c031fbd0d5d41a8cac4bbd3994ea9d5537b4e2576e7a0fdc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
